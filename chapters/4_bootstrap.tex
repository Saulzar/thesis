\chapter{Model Assisted Bootstrapping for Annotation of Segmentation Datasets}
\label{chap:bootstrap} 
 
 
\section {Introduction}

The work in this chapter was published in \cite{Batchelorh}. In this chapter, I describe experiments with image segmentation, where the \gls{VBA} method central to this thesis was initially used as a time-saving exercise and considered among other ideas such as the use of superpixels, partial annotation, and use of existing interactive segmentation methods (e.g. scribbles).

I present a bootstrapping method for creating a segmentation dataset guided by a feedback loop using a human annotator and a model (a \gls{CNN}). Segmentation is picked as a task because it applies to many domains and is very flexible; for example, labelling tree branches using axis-aligned bounding boxes is infeasible because of the irregular shape. This approach is based on using a partially trained model to assist the human annotator.

 A partially trained model's prediction is used as a starting point for a human annotator to verify and refine. The approach is demonstrated by applying these ideas to building a small segmentation dataset for labelling trees in a plantation. 

I show that by using transfer learning by fine-tuning a pre-trained model, very few hand-annotated images are necessary to provide a good level of assistance to a human annotator, using the related idea that recognition is better than recall. Just as it is much faster to recognise a solution rather than recall one from memory, if a model can produce a mostly accurate annotation, a human can recognise and fix small problems much faster than reproducing the annotation from scratch. At worst, the human annotator can discard the automated solution and produce the annotation by hand. This method provides the added benefit of providing visual feedback to an annotator, showing progress on each unseen image.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{bootstrap/trees_example.png}

\caption{Example from the \emph{trees} dataset}
\label{fig:bootstrap_tree}
\end{figure}

\subsection{Related methods}

There are many approximate and interactive machine methods specific to segmentation in addition to those laid out in chapter~\ref{chap:introduction}. Each pixel in a mask is highly correlated with its neighbourhood pixels, as such inputting each pixel is highly redundant; therefore, smart methods can be used to extrapolate segmentation masks from more approximate input. In addition, approximate mask labelling is usually sufficient for most purposes, such as the use of polygons instead of precise boundaries.

An example of active learning specific to segmentation is \cite{Xu2017}, where they focus on finding the nodes (superpixels) which induce the largest change in a \gls{CRF} model.

Semi-supervised machine learning attempts to use existing knowledge or domain properties to infer annotations. For example, using motion cues to give indications of object boundaries \cite{Hong2017}, or using an image classifier to perform object detection by blanking out portions of the image, to determine which parts are important \cite{Bazzani2016}. Semi-supervised methods often substitute for human labour, but in doing so, make sacrifices on the quality and often result in somewhat noisy data. As a result, semi-supervised methods are often used as a means of bootstrapping a model before involving a human annotator, for example as used in \cite{Papadopoulos2016}.


\subsection {Assisted annotation}


\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/labelme.png}
  \caption{Labelme mask annotation}  
  \label{fig:bootstrap_labelme}
\end{subfigure}% 
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/superpixels.png}
  \caption{Annotation using superpixels}
  \label{fig:bootstrap_superpixels}
\end{subfigure}

\caption{Assisted annotation methods}
\label{fig:bootstrap_annot_method}
\end{figure}


In the initial attempts to annotate a dataset, one of the problems was that using the assisted annotation tools, such as the mask tool in the LabelMe \cite{Russell2007} interface, actually required more work than manually annotating the polygon boundary from scratch. The LabelMe mask tool provides a GrabCut-esque interface where positive and negative stripes can be painted onto an image. Attempts to creating a mask for a tree trunk can be seen in Figure~\ref{fig:bootstrap_labelme}, where annotations are bright red and green. The initially painted stripe provided a rough outline, but many small annotations were needed to make the mask resemble the accurate boundary of the tree trunk.

In \cite{Galloway2017}, the images are segmented using superpixels (clusters of pixels grouped with an unsupervised algorithm), where labelling groups of pixels is substantially less work than labelling individual pixels, and superpixels seem to do well at finding image boundaries where \gls{CNN}s are often imprecise. One difficulty is ambiguity, where multiple classes are covered in one superpixel. In my experience using superpixels provided imprecise boundaries, and saved little time. An example of the tool with superpixel selection is shown in Figure~\ref{fig:bootstrap_superpixels}.

Deep learning has previously been used to aid in object instance selection. In \cite{Xu2016} a \gls{FCN} is trained to perform semantic segmentation of objects in images as selected by users. Users click on an object (providing positive and negative points), which provided to the model as distance maps along with the regular colour image data. In \cite{Xu2017} the approach is similar, using a rectangle as an input (also using a distance map), and producing an instance segmentation.


\section{Proposed Method}

The approach is to annotate data and train a model concurrently. First annotate a small number of examples to begin with, to create an initial training set, and use these examples to train an initial model. New examples are then classified by the model, at which point the human annotator fixes any mistakes, and the corrected example is added to the training set. Initially, the annotator will need to fix the majority of the data, and as the model improves such feedback will be required less and less. The idea is that the model will quickly learn the easy cases, which can be quickly ignored to save work, and the annotator will be left to intervene only with the more important challenging cases.

I have used this method to build a small scale tree trunk segmentation dataset, a prototype for use in an automated drone tree pruning project with the aim of pruning lower branches off young \emph{Pinus radiata} tree plantations. This work is intended to allow the drone to see the trees despite the forest. To evaluate methods of dataset construction and evaluate methods of annotation, I wrote an annotation application written with a Qt interface, with tools for drawing on masks and iterating the dataset and refining model proposed annotations.

The work-flow after the first training of some initial  examples was to leave a training process in the background, which would pick up newly created annotations after each training epoch. 

\subsection {Models}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{bootstrap/network.pdf}
  \caption{Encoder--decoder network with pre-trained ResNet as a backbone}  
  \label{fig:bootstrap_network}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{bootstrap/network_blocks.pdf}
  \caption{Sub-networks. On the left the decoder which combines a low resolution feature map (from a deeper level of the network), with a feature map from the backbone ResNet. On the right the basic residual block with two $3\times3$ convolutions. }  
  \label{fig:bootstrap_decode_block}
\end{figure}

For the image segmentation model, I have used an encoder-decoder with skip connections similar to UNet \cite{Ronneberger2015} (utilising padding and without cropping the skip connections). Each level of the encoder and decoder (at the same resolution) is connected by a skip connection bypassing the lower layers of the network. I focus on two architectures, the first is a pre-trained ResNet model as an encoder with a decoder added, and the second is a symmetrical encoder and decoder. I refer to the first as \emph{pre-trained} and the second as \emph{ladder} in the experiments below.

The architecture based on the pre-trained ResNet \cite{He} is shown in Figure~\ref{fig:bootstrap_network}, with the decoder and residual block used shown in Figure~\ref{fig:bootstrap_decode_block}. The particular ResNet I use is the \emph{ResNet18} from the PyTorch \cite{Paszke2017} model zoo, (the smallest of the various ResNet architectures) with weights trained on the ImageNet dataset. 

The decoder for the pre-trained model uses a single residual block at the point labelled \emph{residual block(s)} in Figure~\ref{fig:bootstrap_decode_block}. The ladder model uses residual blocks of size (1, 2, 3, 3, 3, 3) in both encoder and decoder. The decoder uses $ 1x1 $ convolutions to reduce the feature sizes coming from intermediate layers of the ResNet, the number of features used in the decoder is 16, with 8 additional features at each layer further down. An implicit Batch Normalisation layer and ReLU is present before every convolution.

Of note, the first skip connection is directly the image input, and the second is directly after the $7x7, stride=2$ first convolution used by the ResNet.


\section {Experiments}


\subsection {Loss Function}


I primarily use the \gls{IOU} as a measure of comparison for segmentation. The \gls{IOU} is presented in Equation~\ref{eq:iou} with $ A $ and $ B $ being sets. When training a model, I optimise the Jaccard Distance as a continuous approximation to the \gls{IOU}, this is shown in Equation~\ref{eq:jaccard}.  $ p $ is a binary prediction vector of all output pixels, normalised using the sigmoid function to be between 0 and 1. $ t $ is the binary target vector. Multiplication in these equations is element-wise vector multiplication.


\begin{equation}
IOU = \frac{A \cap B}{A \cup B}
\label{eq:iou}
\end{equation}


\begin{equation}
Jacc = 1 - \frac{p.t}{p.p + t.t - p.t}
\label{eq:jaccard}
\end{equation}



\subsection {Image preparation}

I use images of a fixed size in order to train the network (to process images in batches), however, because the network is a fully convolutional network Inference (and testing) can be performed on images of variable size. I show the effect of this processing later, as compared to training with full-size images with batches of size one.

Data augmentation is used to add variety. I use random scales ($0.8$ to $1.25$), and crops and rotations ($-5$ to $5$ degrees). I adjust colours on a per colour channel basis ($ \gamma = 0.9 $ to $ \gamma=1.1 $ )  $ x_a = x^{\gamma} $.

After scaling and rotation, I then crop an area of the image of $440 \times 440$ pixels (the original image size in the \emph{trees} dataset is $800 \times 600$, down-scaled from the original photos of approximately 25 megapixels).

I employ image whitening as the last step, subtracting an approximate global mean (r, g, b) $ (0.485. 0.456, 0.406) $ and dividing by standard deviation $ (0.229, 0.224, 0.225) $  to ensure consistency with the pre-processing used in ImageNet training with the pre-trained model.



\subsection {Datasets}




I developed a small scale dataset for segmentation of tree trunks in \emph{Pinus radiata} forests (using the approach described). The dataset consisted of 120 images, each containing multiple instances of tree trunks. I labelled the most distinct instances in each image, where some images contained hundreds of background trees. Of the 120 images, I used 30 images for a validation set to use in experimentation in this chapter (which were not used for training).

Along with the \emph{tree} dataset, single classes from the Pascal VOC dataset are used in some experiments. I train with a mix of images from MS COCO \cite{Lin2014} training set along with the Pascal VOC 2012 training set, and use the VOC validation set for validation. The tree images typically contain more instances per image at higher resolution, but significantly fewer images than VOC and COCO images for each category.


\subsection {Minimum viable examples}

\begin{figure}[ht!]
\begin{subfigure}[t]{1.0\textwidth}
  
\centering
\includegraphics[width=0.9\linewidth]{bootstrap/limit.pdf}
\caption {Validation with a limited number of images, solid lines are using the pre-trained model}
\label{fig:bootstrap_limited}
\end{subfigure}


\begin{subfigure}[t]{1.0\textwidth}
  \centering
 
  \begin{tabular}{ l  l  l}
    examples & training epochs (of 256 images) & repetitions \\
    \toprule
    1       & 8     & 10 \\
    10       & 32     & 4  \\
    100   & 64     & 2 \\
    1000  & 128 & 1 \\
    \bottomrule
  \end{tabular}
  
  \caption{Training parameters for different numbers of examples}

\label{fig:bootstrap_limit_params}
\end{subfigure}

\end{figure}





The effectiveness of involving the model early in the process will be determined by the point where fixing errors in the model's predictions becomes faster than annotating the image from scratch. The effect of the number of examples was studied in \cite{Soekhoe} in the context of transfer learning. One finding was that with smaller numbers of examples, fixing the weights of earlier layers boosted performance when using less training examples.

I performed a simple validation experiment to investigate the effect of using fewer examples. The results can be seen in Figure~\ref{fig:bootstrap_limited}, where in each case the stated IOU is an average over several trials (the number can be seen in table~\ref{fig:bootstrap_limit_params}), and is additionally averaged over the last three epochs of training. Here I have used classes from the Pascal VOC 2012 for comparison.

It can be seen that even for as few as 10 examples, the validation accuracy is not too different from using a full dataset (1000s of images) in each case, and the difference between using 100 images and the full dataset was minimal in each case. I expected to see the pre-trained model perform much better with a limited number of examples, and this proved to be the case,  it also seems to perform much better in the general case as well. The case matching the initial expectations is the (much more limited) \emph{trees} dataset.



\subsection{Model fine-tuning}

\begin{figure}[!ht]
\centering
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/fine_tuning.pdf}
  \caption{Influence of learning rate modifier for fine-tuning}  
  \label{fig:bootstrap_fine}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/depth.pdf}
  \caption{Depth of pre-trained model}  
  \label{fig:bootstrap_depth}
\end{figure}


I examined the best parameters for fine-tuning, finding that allowing a small learning rate on the pre-trained model was advantageous. The effects can be seen in Figure~\ref{fig:bootstrap_fine}, where $0.01$ (fraction of the learning rate used for other parts of the model) was advantageous. I used $ 0.1 $ for all other experimentation in this chapter. It can be seen that without the lower learning rate, the pre-trained parameters are disturbed leading to lower validation accuracy, and without any fine-tuning, the pre-trained parameters are not able to adapt.

Depth of the model is examined, and I experiment with truncating the pre-trained model. Results can be seen in Figure~\ref{fig:bootstrap_depth}, where the depth of the model had a strong positive effect on the output of the model. Given these results, I might consider using a model with even more depth scales. Given the number of convolutions (especially at the lower scales), the ResNet--18 model I have been using has quite a large receptive field, and with more depth scales the receptive field would be larger. In \cite{Peng2017} benefit was found in using very large kernels at the lower levels of the model specifically to enlarge the receptive field.






\subsection {Partial Annotation}


\begin{figure}[!ht]
\centering
\begin{subfigure}[t]{.33\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/loose.png}
  \caption{Partial annotation}
  \label{fig:bootstrap_loose_annot}
\end{subfigure}%
\begin{subfigure}[t]{.33\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/loose_directed.png}
  \caption{Directed annotation}
  \label{fig:bootstrap_loose_dir}

\end{subfigure}%
\begin{subfigure}[t]{.33\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{bootstrap/loose_predictions.png}
  \caption{Partial inference}
  \label{fig:bootstrap_loose_pred}
\end{subfigure}
  \caption{Loose annotation methods in (a) and (b), the red overlay is pixels labelled as background where transparent pixels are unlabelled, (c) shows the kind of noisy outline produced from partial annotation }


\end{figure}

\begin{figure}
\centering
\begin{subfigure}[t]{.33\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{bootstrap/line_tool.png}
  \caption{Line tool}
\end{subfigure}%
\begin{subfigure}[t]{.33\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{bootstrap/polygon_tool.png}
  \caption{Polygon tool}
\end{subfigure}%
\begin{subfigure}[t]{.33\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{bootstrap/paint_tool.png}
  \caption{Paint tool}
\end{subfigure}%

  \caption{Label paint tools used in annotation}
  \label{fig:bootstrap_tools}

\end{figure}



An alternative to densely annotating every pixel in an image is to selectively label some pixels while leaving many unlabelled - in the vein of the GrabCut algorithm. I examine this idea for two reasons, firstly annotation of all pixels is finicky or ambiguous (such as the small trees in the background), and secondly to optimise time spent on the annotation of only the most informative pixels (in the vein of active learning).

I firstly looked at partial annotation using paintbrush scribbles on the centre of trees and around the edges of the background. It can be seen that the model trained with partial annotation does not accurately find tree boundaries and has reduced precision as a result, as can be seen in Figure~\ref{fig:bootstrap_loose_pred}. After identifying flaws in that process I compared directed annotation, where I trained a model as I went, in order to guide the annotation process.

For directed annotation, I aimed to densely label the tree instances only where there were mistakes, otherwise leaving the rest of the image not annotated. At the points where mistakes occurred, I made sure to capture the boundaries more accurately. I was again annotating only a small number of easier instances where annotation was clear, unlike in the dense annotation case where many small and ambiguous instances are labelled.

Some statistics can be seen in table~\ref{tab:loose_exp}, where I can see that both methods of partial annotation are less effective than dense annotation. The second method of directed annotation improves on the precision but reduces recall. It can be seen that the extra supervision of background pixels is indeed useful, and useful supervision is lost by not including it.


\begin{table}[!ht]
  \centering
    \caption{Statistics from re-annotation test set}
    
  \begin{tabular}{ l  l  l l l l l }
    method & positive & background & ignored & precision & recall & IoU  \\
    \toprule
    dense annotation    & 10.4\% & 89.6\% & 0\% & 87.8 & 87.3 & 77.8 \\
    loose annotation    & 5.2\% & 67.4\% & 27.4\% & 82.3 & 83.3 & 70.7 \\
    directed annotation & 7.2\% & 54.9\% & 38.0\% & 87.7 & 80.7 & 72.5 \\    
    \bottomrule
  \end{tabular}

\label{tab:loose_exp}
\end{table}

\subsection {Annotation interface experiment}

I performed an exercise in re-annotating the validation set using different tools. The results can be seen in table~\ref{tab:annotation_exp}, where I used three approaches. The different tools used can be seen above in Figure~\ref{fig:bootstrap_tools}.  The first being a duplication of the initial annotation using the line tool (the tool I initially used to label the dataset), the second being a polygon tool (more commonly used and general purpose than the line tool), and lastly I refined images produced by a trained model, fixing mistakes using the paint tool.

The model I used was a model trained on 30 images (densely labelled) from the loose annotation experiment above. I can see the method of refining model predictions is somewhat faster than the other two input methods, but due to the exercise being undertaken by a single user it is not significantly different ($ p > 0.01 $).

During the annotation, I showed an image of the existing validation image and its annotations side by side to the annotation software, which was necessary to ensure the same task was being performed. The smaller instances in each image were subject to the judgement of the annotator, and other images were extremely crowded or blurry, leading to ambiguity in the annotation process. 

I consider human variability, using a reference annotation created with the same tool operated by the same user. The annotation compared to the reference, only manages a precision and recall of approximately $90\%$. All three input methods were similar in this regard, but display different bias. The polygon tool, for example, was more precise but less complete (lower recall), indicating that the user tended to cut within boundaries defined by the original labelling. 
 
\begin{table*}[!ht]
  \centering
    \caption{Statistics from annotating the validation set in different ways. Precision, recall and IOU are a comparison with the original validation set. Note figures in brackets are the original statistics of the unmodified predictions from the model}

\noindent\resizebox{\textwidth}{!}{%    
  \begin{tabular}{ l  l  l  l  l  l  l  l }
    method & \shortstack{time \\ per image (s)} & \shortstack{edits \\ per image (s)} & \shortstack{pixels \\ per image} & \shortstack{time \\ per edit (s)} & precision & recall & IoU \\
    \toprule
    polygon    & $79.3 \pm 35.0$  & $12.1 \pm 5.6$ & $52438 \pm 31900$   & $3.9 \pm 1.8$  & $0.94$ & $0.88$   & $0.84$ \\
    line         & $71  \pm 33.4$   & $12.2 \pm 5.6$ & $54781 \pm 33336$   & $2.1 \pm 1.4$ & $0.92$  & $0.90$  & $0.84$ \\
    refine     & $57.3 \pm 40.3$  & $10.8 \pm 6.1$ & $6561 \pm 7102$       & $1.7 \pm 1.4$ & $0.92 (0.89)$   & $0.92 (0.85)$ & $0.85 (0.77)$ \\    
    \bottomrule
  \end{tabular}
}

\label{tab:annotation_exp}
\end{table*}


The confounding feature in the annotation experiment was the need to look backwards and forwards between the two images in order to ensure the correct areas were annotated. In images with many instances, this became a difficult task and identifying small differences between the two became much more difficult. In future, I may perform a more comprehensive user study, by providing an outline of the desired mask for the user to copy directly. This way, the user does not spend any time comparing images and can focus only on the interaction (as a regular user of the annotation tool would).


\subsection{Practical considerations}

Hyperparameters, for example, training rate, model architecture, still need to be determined by validation as soon as enough examples have been obtained to be feasible. I performed some tuning of the hyperparameters (for example the crop size, data augmentation parameters, learning rate) for the purposes of the experiments in this chapter using the person category of the Pascal \gls{VOC} 2012 dataset, despite the fact that the \emph{tree} dataset is significantly faster to train a similar set of hyperparameters seems to have been suitable.


Practical considerations of having a training process for a \gls{CNN} in the background is that it uses a lot of both \gls{CPU} and \gls{GPU}) resources. Running the trainer and the annotation tool together caused significant lag in the annotation tool, which was not otherwise present. It was especially noticeable with a large batch size when the tool would lag in sync with each batch processed. The effect was that annotation became more time-consuming. A simple solution to this would be, for example, to run processes on different machines with a web interface.


\section{Future work}

The interest in tree segmentation was initially for robotic pruning applications and navigation. For this purpose, the segmentation requires further interpretation and fitting. For example, in order to identify tree instances, groups of pixels can be identified then fitted to oriented boxes. This approach does not work for overlapping trees; however, object detection methods also typically struggle with highly overlapping objects.

For these reasons the direction I pursued was to take some of the ideas and lessons learned and apply them to object detection, as can be seen in chapters~\ref {chap:object_detection} and~\ref {chap:annotation}.

For purposes such as identifying foliage of the trees or looking at obstacles in the forest, segmentation does seem like the most natural fit; where one object cannot be easily separated from another. For example, the \emph{Pinus radiata} needles are not distinguishable in the canopy as to which tree they are attached.

\subsection{Segmentation specific}

There is considerable scope for assisted annotation of segmentation datasets, specifically, because the annotation of each pixel is especially laborious. Here are some potential future directions: 

\begin{itemize}
    
\item  {\bf Interactive segmentation} \par
Segmentation often has a class-agnostic element, where the boundary of an object is clear regardless of the object type. User hints can often resolve any ambiguity. An example is interactive object selection \cite{Xu2016, Xu2017} where user input is combined with image data to segment an object. Large segmentation datasets combined with simulation are used for training, however, an iterative refinement on the specific dataset during annotation may bring further benefit.

\item  {\bf Object selection} \par
Object selection algorithms that can work on the features or outputs from \gls{DNN}s is another idea, such as higher dimensional GrabCut \cite{Xu2016a}, or SuperPixels. Performing Features learned from pre-training on large image datasets is also possible.

\item  {\bf Polygons or shapes}
Pixel masks are rarely fully accurate, and drawing tools provide approximate methods for inputting masks in the first place. Directly predicting the kinds of shapes used to input annotations might be a better alternative. Polygons, for example, used in PolygonRNN \cite{Castrejon2017}. Like pixel masks, polygons can be produced by a \gls{NN} and refined by a human annotator. Polygons can be manipulated more easily and better represent the fact that the boundary is an approximation.
\end{itemize}

\section {Conclusion}

I proposed a bootstrapping method for rapid segmentation dataset creation, involving a feedback loop between human and model. The model trained on partial data was used to assist a human annotator, who then verified the model's predictions and fixed any mistakes. I used this method to create a \emph{tree} dataset for segmentation of \emph{Pinus radiata} trunks (to find trees requiring pruning). I then performed several experiments on this dataset, as well as the Pascal VOC in order to see how I might improve on this bootstrapping method. 

Through several experiments, I determined that fine-tuning a pre-trained model was an effective way to obtain a workable model with very few input images, at least on the \emph{tree} dataset. Partial annotation was explored and found to be less effective than a dense annotation. For this reason, I think that it is more useful to create tools to assist the user to create a dense annotation, rather than making do with more approximate labelling. 