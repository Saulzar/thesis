
\chapter{Object Recognition by Stochastic Metric Learning}
\label{chap:metric} 

Descriptors extracted from deep neural networks have been shown to be very discriminative. Networks such as those trained on the large, very general ImageNet dataset have been used to extract descriptors which can then be used robustly for a variety of image classification tasks. Such retrieval systems utilise feature locality, for example, Approximate Nearest Neighbour. The goal is to use such descriptors as part of a large scale object instance recognition and retrieval system. I propose using deep nonlinear metric learning on Convolutional Neural Networks to learn features with good locality. In particular, I worked with two related methods, \gls{NCA} and the related \gls{MEGM}.

I utilise a nonlinear form of \gls{MEGM} as an alternative to \gls{NCA} and propose some stochastic sampling methods to apply these batch learning methods to larger datasets with mini-batch \gls{SGD}. On a larger scale, I found the methods challenging to train, failing to converge or generalising poorly depending on the training method or parameters. This led to returning to a smaller dataset and examine the factors which lead to good generalisation with this form of training.
  
Surprisingly, on a small subset of the RGB-D dataset, stochastic sampling methods generalised much better with small batch sizes (they acted as a form of regularisation). When trained with larger batches, or as a full batch, the dataset was overfitted. Given the correct parameters, extracted descriptors performed well at the Nearest Neighbour task and exceeded the performance of those extracted by applying standard supervised training.

\section{Introduction}

Deep convolutional neural networks in combination with modern \gls{GPU}s and large image datasets have shown strong performance on image classification tasks \cite {Krizhevsky2012}, and have been applied to related problems such as object detection \cite{Sermanet2013}, image segmentation \cite{Masci2013} and image retrieval \cite{Razavian2014}.

\subsection {Descriptors from Deep Neural Networks}

Using descriptors, derived from the hidden layers of neural networks trained for classification, for the purpose of other learning tasks, is a relatively new idea. These descriptors have been shown to be robust even for quite unrelated tasks \cite{Donahue2014,Razavian2014}. The ImageNet dataset \cite{Krizhevsky2012} is a popular source for pre-training, and pre-trained models exist such as the OverFeat network \cite{Sermanet2013} or the DeCAF feature extractor \cite{Donahue2014}). 

A standard technique in training a \gls{CNN} is to augment the dataset by applying transformations, as more data typically gives better generalisation. In \cite{Dosovitskiy2013}, a \gls{CNN} was trained on single images which were warped in many different ways. Features obtained from the network were then used in popular classification benchmarks achieving good results. For many years local image descriptors such as \gls{SIFT} \cite{Lowe2004} have been used for matching and indexing images.  A recent comparison \cite{Fischer2014} (though perhaps not a fair one) showed that using a \gls{CNN} for matching tasks performed better than \gls{SIFT} by a margin similar to the improvement given by \gls{SIFT} to raw pixel data.


The final layer of a standard deep neural network as used in supervised classification consists of a set of linear classifiers, such as those descriptors that are suitable for classification using other linear classifiers, for example, \gls{SVM}s. Nearest Neighbor suffers from high dimensionality and noisy or irrelevant dimensions, so the descriptors produced by a CNN may not be suitable for comparison by distance. For that reason, I have looked towards metric learning to directly optimise the descriptors for Nearest Neighbor classification. 


\subsection {Deep Metric Learning}


Metric learning has often been used for object recognition, and image classification \cite{Hadsell2006,Min2009} (and many others), and especially face recognition, for example, \cite{Kostinger2012}. Although most efforts have often focused on Mahalanobis distance metric learning (a form of distance metric learning linear transformation), deep metric learning has had some attention \cite {Salakhutdinov2007a,Min2009,Weston2009,Min2010}. At the expense of a much larger computation cost, deep metric learning has been shown to perform much better than its linear counterparts. Gradients from metric learning are used to drive \gls{SGD} on a deep \gls{CNN}. 

Functions for metric learning often focus on pairs or triples, using siamese networks where parameters are shared between two, or three branches of identical networks. This method generalises this to multiple examples at a time.

\subsection {Training}

Metric learning comes with its own set of challenges, and it has often been formulated as a batch training method because each example potentially interacts with every other example. In practice, descriptors from examples far apart do not interact with each other at all, so approximations can be made, as discussed later. High dimensional spaces from feature vectors can face issues, namely the ``curse of dimensionality''. In high dimensional spaces, evenly spaced points (descriptors) increase in the number of neighbours with increased dimension. 

The interaction between points decays with distance (for example, exponentially with \gls{NCA}). I can use approximations around the local neighbourhood of examples which can be used to create a \gls{SGD} training procedure. Using an approximation to the nearest $ k $, neighbours is an approach seen in \cite{Mensink2012,Zaidi2011} (among many). Clustering (amongst other sampling methods) is discussed in  \cite{Oneat2011} such as Farthest Point or Random Projection clustering, but the downside of such clustering is that it is hard to control the size of a batch. 

Many training methods focus on the interaction between pairs of (similar/dissimilar) examples or triples (example, more similar, less similar), for example, DrLIM \cite{Hadsell2006} where a spring analogy is used to create an attraction between similar pairs and repulsion between dissimilar pairs. The advantage of pairwise methods is that they can be used with the weaker labelling of similar/dissimilar (as opposed to category label).

\section {Deep Metric Learning}

A deep neural network (in this case a \gls{CNN}) is used to embed images into a lower dimensional space, creating descriptors which can be compared with their Euclidean distance (and classified with nearest neighbour) where the Euclidean distance of the raw pixels is both computationally expensive and not a good measure of the distance of the semantic similarity of image content. 

I examine non-linear versions of two methods \gls{NCA} \cite{Goldberger2004}, and the closely related, but less well known \gls{MEGM} \cite {Zaidi2011}. \gls{NCA} optimises a continuous version of the \gls{LOO} performance, and it uses a softmax over weights which decay exponentially with distance. The \gls{NCA} score can be interpreted as the probability that a descriptor will pick another descriptor of the correct class as its nearest neighbour. 

The probability, $ p_{ij} $, of one descriptor selecting another descriptor as its neighbour, is defined as a softmax function over weights $W_ij$. The indexes $ i $ and $ j $ refer to input examples $x_i$ and $x_j$, and corresponding vector valued output of a \gls{CNN} $f(x_i)$ and $f(x_j)$ which are the descriptor vectors.

\begin{equation}
\label{eq:nca_prob_pair}
p_{ij} =  \frac {W_{ij}} {\sum_{k \neq i}{W_{ik}}}g
\end{equation}

Then the total probability, $ p_i $, of a point selecting any neighbour with another with its \emph{correct} class is defined as the sum of those neighbour probabilities $p_{ij}$ which have the same class:

\begin{equation}
\label{eq:nca_prob}
p_{i} =  \sum_{j:c_j = c_i}{p_{ij}}
\end{equation}

Where $ C_i $ is the class label of example $ i $. a Gaussian kernel is used for the weighting as \cite{Zaidi2011} do. 

\begin{equation}
 \label{eq:gaussian_kernel}
W_{ij} = exp(\frac{-\lVert f(x_i) - f(x_j) \rVert^2_2 }{2\sigma^2}), \space W_{ii} = 0
\end{equation}


The function to be maximised is the sum of the probabilities of all descriptors being correctly classified.

\begin{equation}
\label{eq:nca_loss}
\mathcal{E}_{nca} =  \sum_i {p_i}
\end{equation}

Where \gls{NCA} optimises directly on the probability $ p_{i} $ above, \gls{MEGM} instead computes for each class $ \hat{y_{ti}} $ as a prediction that a descriptor will take class $ t $, where the only difference is that $ c_j = t $ as opposed to $ c_j = c_i $:

\begin{equation}
\label{eq:megm_pred}
\hat{y_{ti}} = \frac{\sum_{j:c_j = t}W_{ij}}{\sum_{k \neq i}{W_{ik}}}
\end{equation}

The prediction $ \hat{y_{ti}} $ can then be compared with $ y_{ti} $ (1 where $ t = c_i $, 0 otherwise), and it then minimises the \gls{MSE} between prediction and true class label:

\begin{equation}
\label{eq:megm_loss}
\mathcal{E}_{megm} =  \sum_i\sum_t{(y_{ti} - \hat{y_{ti}})^2}
\end{equation}

Intuitively \gls{MEGM} can be seen to penalise the case where two classes compete for the same region,  more so than when one class competes against examples of many different classes, whereas \gls{NCA} would treat the two cases approximately equally. These loss functions can be used to drive gradient descent on a \gls{CNN} by standard back-propagation. The derivative for \gls{MEGM} is shown in the appendix, section~\ref{sec:appendix}.

The gradient is computed over the outputs of each mini-batch and apply back-propagation to find the derivative with respect to the weights of the network. It can be noted that the output and derivative for \gls{MEGM} is more expensive to compute because of the additional per class summation, so it would not be suitable with an extremely large number of classes. In practice, a large number of the terms can be factored out and pre-computed, as well as computing the difference summations in terms of matrix multiplication.

Note the parameter $ \sigma $ was not in the original \gls{NCA}, and is initialised to the average distance to the nearest neighbours of the initial descriptor output before training. It is used to prevent the weights initialising to zero when the distance between descriptors is large.


The parameter $\alpha $ controls the trade-off. When $ \alpha \mathcal{E}_{mse} > \mathcal{E}_{nca} $ the descriptors all collapse into the same point.


\section{SGD for metric learning}

The main proposal is in using mini-batch \gls{SGD}, and applying it to metric learning methods which have been designed as batch learning methods. Metric learning as shown above as a batch method, scales at $ O(n^2) $ for $ n $ examples. Given that the desire to apply these approaches to large datasets containing hundreds of thousands or millions of images, I am forced to consider approximations. The typical method for training a \gls{CNN} on large numbers of images is using \gls{SGD}, because it is fast, simple and scales to handle large datasets easily. 

The most obvious approximation is to truncate the influence to the nearest $ k $ neighbours as the weight exponentially decays with the square distance. It is hypothesised this would lead to the best approximation, however, there are many ways of truncating the neighbourhoods. This lends itself to clustering methods and is more complicated than the alternative, which is to sample batches randomly but uses a large enough batch to include several examples of each class.

I propose the following approaches for sampling batches for \gls{SGD}:

\begin{enumerate}
\item {\bf Random shuffled batches} \par
 Randomly shuffle the dataset and divide it up into batches of a fixed size, which is exactly how batches would normally be selected for supervised learning. Each batch contains (almost certainly) different numbers of examples from each class.
 \item {\bf Stratified random batches}  \par
 Pick batches by selecting a number of examples from each class, to ensure the same number of examples of each class are represented in each batch.   
 \item {\bf K-neighbourhoods around random points}  \par
 Before each training epoch, run the model forward through the training set to obtain descriptors for each example. Select N examples at random and pick the batch as the batch sized neighbourhood (in descriptor space) of each selected example.
\end {enumerate}


\subsection {Issues of Scale}

The parameter $ \sigma $ is optional in theory, as the scale can be factored into the final fully connected weight matrix (or previous layers).  I make use of $ \sigma $ for numerical stability upon initialisation, and during training, the density of points adjusts itself to fit this parameter. It can also be observed that for different values of $ \sigma $ that the distance between descriptors adjust themselves to fit the new parameter over a few iterations of training.


\subsection {Adding Mean Square Error}

I experimented with adding the square distance between members of the same class as a means of adding some bias to the loss functions after suspecting that metric learning methods described above were overfitting, to force the output distribution to be more simple. 

\begin{equation}
\label{eqn:mse}
\mathcal{E}_{mse} = \sum_i{ \sum_{j:j_c = i_c}  \frac {{\lVert f(x_i) - f(x_j) \rVert^2_2}} {\sigma^2} }
\end{equation}

\begin{equation}
\label{eqn:mse_total}
\mathcal{E}_{total} =  \mathcal{E}_{nca} + \alpha \mathcal{E}_{mse}
\end{equation}


\subsection {kNN implementation}

I use a brute force \gls{KNN} algorithm written in CUDA \cite{Garcia2008} for the \gls{GPU}, computing the distance matrix using matrix multiplication followed by using an insertion sort to select the $ k $ neighbours of lowest distance. This approach is not scalable to large datasets, and smarter clustering algorithms will eventually need to be used; however, the time for evaluating \gls{KNN} on the datasets I experimented with, are still dominated by the cost of computing descriptors from examples. 


\section {CNN architecture}


\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{metric_learning/convnet.pdf}
\caption{Convolutional network configuration used for $64\times64$ RGB images with supervised learning.}
\label{fig:metric_convnet}
\end{figure}

I used a simple convolutional neural network of six layers, with four layers of convolution and max-pooling using rectified linear activation functions, with two fully connected layers using Maxout \cite{Springenberg2013} units as an activation method, shown in Figure~\ref{fig:metric_convnet}. Dropout \cite{HintonDropout} with a rate of $ 0.5 $ is used when training on inputs to the two fully connected layers. For metric learning, the last linear layer and softmax are removed, leaving four convolutional layers and a single fully connected layer giving descriptors of size $ 256 $.

Dropout and Maxout have been shown to be beneficial in a supervised learning scenario for the purposes of regularisation. In the standard supervised training scenario Dropout is of practical use because it (to some degree) prevents overfitting, and mostly does away with the need for early stopping. However, I found that it interfered with generalisation when used with metric learning approaches.

\subsection {Data augmentation}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{metric_learning/augmentation.png}
\caption{Example of image distortions resulting from transformations of a single source image}
\label{fig:metric_augmentation}
\end{figure}


In all cases, I used randomised data augmentation of the test set by applying random distortions to ensure the network never saw exactly the same image twice, and to increase its tolerance to small changes in lighting, translation and rotation. The parameters of the data augmentation can be seen in Figure~\ref{fig:metric_permute}. Without the data augmentation, supervised training produces substantially worse generalisation than without, in both supervised and metric learning approaches. In all experiments, testing was performed on non-augmented images and trained with augmented images.

\begin{table*}
  \centering
    \caption{Ranges of parameters used for image augmentation }

  \begin{tabular}{ l  l }
    \toprule
    scale (uniform) & $ 1 \pm 0.2 $  \\ 
    squash  & $ 1 \pm 0.2 $  \\ 
    rotation (rads) & $ \pm \frac{\pi}{16} $ \\ 
    translation(x, y) (\% image size) & $ \pm 5 \% $ \\ 
    brightness (additive) & $ \pm 20 \% $ \\ 
    contrast (multiplicative) & $ 1 \pm 0.2 \% $ \\ 
    gaussian pixel noise & $ \sigma = 2 \pm 2 $  \\ 
    flip horizontal (probability) & $ 0.5 $ \\ 
    \bottomrule
  \end{tabular}
\label{fig:metric_permute}
\end{table*}

\subsection {Dataset}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{metric_learning/objects.png}
\caption{Samples of images of different objects used in training}
\label{fig:metric_dataset}
\end{figure}

I experimented with the University of Washington RGB-D dataset primarily because it has a standard test set for instance recognition and a large number of published results. It contains 300 objects from 50 classes. Each object has three sequences of rotations at 30, 45 and 60 degrees elevation. Each rotation sequence contains approximately 150 images. For instance recognition, the sequence at 30 and 60 degrees are used for training, and the sequence at 45 degrees is used for testing. 

A cut-down version of the RGB-D dataset is used for a number of experiments, with 50 objects and 100 images of each object to train on, and 50 images per object to test. The images were randomly selected.

The resolution of $ 64\times64 $ on the RGB-D images is used, with the cropped version of the images. The procedure for resizing is to load all of the images in a sequence and if any image is at a higher resolution than $ 72\times72$, I resize the image to $ 72\times72 $. The same factor scales all other images in the sequence. The images are then augmented (see Figure~\ref{fig:metric_permute}) and finally centred (modulo translation) on a blank image at  $ 64\times64 $.


\section {Experiments}

In all cases (unless otherwise specified) I use a mini-batch size of 256, with standard \gls{SGD} learning rate set to $ 10^{-2} $ for supervised learning, and $ 10^{-5} $ for metric learning methods. I experimented with other learning rates for metric learning, and in some cases, a lower learning rate of $ 10^{-6} $ was used when the higher rate caused divergence.

I manually divided the training rate by a factor of 10 when the training set accuracy plateaus for supervised learning. Supervised learning methods greatly benefit from reducing the learning rate after time. However, there was no noticable benefit to the metric learning methods. I stopped metric learning methods at 70 epochs, or earlier if they were not converging. Overall, \gls{MEGM} to gave similar, but slightly better test set accuracy than \gls{NCA}, and most experiments used with \gls{MEGM} for consistency.


\subsection{Overall Comparison}

I compared the testing classification error between methods, and the final accuracy is reported as the test set classification accuracy average over the last 5 iterations. Test set accuracy is percentage accuracy. For metric learning, the class chosen is the most common class in $k = 5$ nearest neighbours.


\begin{table*}[ht]

\centering
  \caption{Summary of training methods}

  \begin{tabular}{  l l  l  l l l }
  
    \toprule
    method &  sampling & batch  & test accuracy &  train epochs \\  \hline
    \bf{Initialisation} & &  &  $ 64.0  $ & 0  &  \\  
    \hline
    
    \bf{Supervised} & & 256 &  $ 90.6  $ & 40  &  \\  
     & 5NN  &  &  $ 89.0  $ & 40 &  \\  
     \hline
    
    \bf{NCA} &  & batch &  $  71.2  $ &  50  & \\
     & random & 256 & $  94.0  $ & 70 & \\
    
    \hline
    
    \bf{MEGM} &  & batch  &  $  74.4  $ &  50  & \\
     & random & 128 &  $  95.0  $ &  70  & \\     
     &  & 256 & $  90.5  $ &  70 & \\  
     &  & 512 & $  81.4  $ &  70 & \\
    
    \hline
    \bf{MEGM} & stratified & 128 & $  {\bf 95.4 }  $ & 70 & \\  
    
     &  & 256 & $  94.6  $ & 70 & \\  
     &  & 512 & $  87.1  $ & 70 & \\  

     \hline
     
    \bf{MEGM} & neighbourhoods & 256 & $  80.4  $ & 70  & \\
    
    \hline
    
    \bf{MEGM + MSE} & stratified & 128 & $  {\bf 95.3 }  $ & 70  & \\

      \bottomrule
    
    \end{tabular}
\label{fig:metric_summary}
\end{table*}



I sought to compare the sampling approximation to the full batch (as opposed to mini-batch) method. As a batch method, \gls{SGD} is not the ideal training method. It was surprising to see that despite the loss function smoothly decreasing (as can be
seen in Figure~\ref{fig:metric_megm_test}), training failed to generalise well to the training set Figure~\ref{fig:metric_megm_loss}.
I anticipated the batch metric learning methods would work best as a batch
method or with larger batches (as closer approximations to the batch method).
It is seen that is not the case, and both the batch method and \gls{SGD} with the larger batch size (512) both failed to generalise well. The same pattern occurred for \gls{NCA}, as well as using the stratified sampling method. The reason for such overfitting is, I believe, that the two metric learning methods have not enough bias to force the network to learn something. \gls{NCA} allows highly complex and multi-modal distributions with many local minima, which provided the local neighbourhood structure fits, are not penalised by its loss function. Smaller batch sizes, however, act as a regularisation, forcing the descriptor outputs into a simpler form.



\begin{figure}[ht]
   \input{figures/metric_learning/megm_e}
   \caption{Loss function for different batch sizes, MEGM loss}
   \label {fig:metric_megm_loss}
\end{figure}

\begin{figure}[ht]
   \input{figures/metric_learning/megm_test}
   \caption{Testing error for different batch sizes, MEGM loss}
   \label {fig:metric_megm_test}
\end{figure}



\subsection{Sampling method}

I compared the three different sampling methods, and most noticeably, the k-neighbourhood sampling method did not converge well. The loss function can be seen to oscillate wildly and then does not reach a local minimum and as can be seen in Figure~\ref{fig:metric_summary} did not produce good generalisation to the test set. Reducing the learning rate did not seem to help in this case. In the same figure, the result of adding in a \gls{MSE} term to the loss function can be shown to provide a slightly faster convergence rate while reaching the same testing classification error. 


\section {Conclusion}

I discovered that metric learning with \gls{NCA} and \gls{MEGM} can produce good results under the right conditions. Used as a mini-batch method, they are sensitive to parameters such as batch size. Large batch sizes caused significant overfitting, while small batch sizes produced the best generalisation, and adding \gls{MSE} increased convergence rate considerably. Of the proposed sampling methods, random batches and stratified sampling worked much better than neighbourhood sampling, which did not converge well.


I validated the proposed idea (at least in the small scale dataset) that the metric learning approach can be used to produce better descriptors than
standard supervised learning, despite the small-sized dataset. Nonlinear \gls{MEGM} generalised a little better than NCA on this particular dataset, with similar
properties.

I believe that neither of these metric learning methods provide enough bias when combined with deep neural networks. They allow
complex (and potentially multi-modal) distributions in the output descriptors, as long as the local neighbourhood structure matches the labelling. I believe this prevented good generalisation in the experiments when the batch sizes were larger.

Pairwise interactions complicate the implementation and I believe contribute largely to the sensitivity of the training process, so make choosing the correct sampling method much more difficult in practice. A simpler alternative I will investigate in future is to choose a fixed descriptor to represent each class like \gls{NCM} \cite {Mensink2012}, avoiding the pairwise interaction as well as forcing the neural network to produce a more general metric.



\section{Appendix}
\label{sec:appendix}

 I adjusted the \gls{NCA} derivative found in \cite {Salakhutdinov2007a} to give the derivative for \gls{MEGM} output for the $ i^{th} $ training case and $ t^th $ class:


\begin{multline}
\label{eq:megm_grad}
\frac{\partial \mathcal{E}_{megm}}{\partial f(x_{ti})} = 
  -2 \bigg( \sum_{j:c_i = c_j}  m_{ti} {p_{ij} \Big( d_{ij} - \sum_z{p_{iz}d_{iz}} \Big) } \bigg)\\
  +2 \bigg( \sum_{j:c_i = c_j} m_{tj}{p_{ji}d_{ji} - \sum_z{\Big( \sum_{q:c_z = c_q}{p_{zq}} \Big) m_{tz}p_{zi}d_{zi}   }} \bigg)
\end{multline}

Where $ err_{ti} $ is shorthand for the partial derivative $ \hat{y_{ti}} $ with respect to \gls{MSE}, and $ d_{ij} = f(x_i) - f(x_j) $ is shorthand for the difference between the descriptor vectors. The formula differs from the \gls{NCA} derivative only by the $ err_{ti} $ term.

\begin{equation}
m_{ti} = \frac{\partial \mathcal{E}_{megm}}{\partial \hat{y_{ti}}} = -2 (y_{ti} - \hat{y_{ti}})
\label{eq:megm_partial}
\end{equation}

