\documentclass[twoside]{styles/uocthesis}

\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage[toc,page]{appendix}

\usepackage[pdftex]{graphicx} %Loading the package
\graphicspath{{figures/}} %Setting the 
\usepackage{mathtools}
\usepackage{bm}
\usepackage[flushleft]{threeparttable}


\usepackage[acronym]{glossaries}
\usepackage{booktabs}
\usepackage{styles/gnuplot-lua-tikz}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{enumitem}

\usepackage{todonotes}
\usepackage{float}

\setlist{parsep=0pt,listparindent=\parindent}


\Title{Verification Based Annotation for Visual Recognition}
\Author{Oliver Batchelor}
\Year{2019}

\Supervisor{Prof. Richard Green}
\Department{Department of Computer Science}

\makeglossaries




\begin{document}

\prelimpages


\titlepage


\abstract{%

Applying deep learning to new domains usually implies a data collection and annotation problem. While several large datasets exist and provide a great deal of utility, there is a need to apply deep learning to new domains more easily, and to more easily experiment without the burden of spending a large amount of time upfront to annotate data. This thesis explores some the ideas and tools needed for rapid prototyping with image data and convolutional neural networks. 
I have focused on visual recognition methods (initially image segmentation, and then object detection) where I've developed ``Human in the loop'' collaborative human--machine annotation methods where both machine and human annotator can be involved at almost every step of the process. 

% One key mechanic is that of verification based annotation, which works if verifying annotation is faster than manually inputting those validations from scratch. Another aspect is example selection, where the machine attempts to find the most informative examples for annotation at each step. Finding mistakes in annotation is usually performed by human verification, I attempt to see how that might 


}

\tableofcontents



\printglossaries

\newacronym{FCN}{FCN}{Fully Convolutional Network}
\newacronym{FPN}{FPN}{Feature Pyramid Network}


\newacronym{FP}{FP}{False Positive}
\newacronym{FN}{FN}{False Negative}

\newacronym{TN}{TN}{True Negative}
\newacronym{TP}{TP}{True Positive}

\newacronym{IID}{IID}{Independent and Identically Distributed}

\newacronym{IOU}{IoU}{Intersection over Union}
\newacronym{VOC}{VOC}{Visual Object Classes}
\newacronym{mIOU}{mIOU}{mean Intersection Over Union}

\newacronym{mAP}{mAP}{mean Average Precision}
\newacronym{AP}{AP}{Average Precision}


\newacronym{CRF}{CRF}{Conditional Random Field}

\newacronym{RELU}{ReLU}{Rectified Linear Unit}
\newacronym{PRELU}{PReLU}{Parameterised Rectified Linear Unit}
\newacronym{DECAF}{DeCAF}{Deep Convolutional Activation Feature}

\newacronym{FPN}{FPN}{Feature Pyramid Network}


\newacronym{NLL}{NLL}{Negative Loss Likelihood}
\newacronym{BCE}{BCE}{Binary Cross Entropy}
\newacronym{CE}{CE}{Cross Entropy}
 
\newacronym{COCO}{COCO}{Microsoft Common Objects in Context}
\newacronym{ILSVRC}{ILSVRC}{ImageNet Large Scale Visual Recognition Challenge}

\newacronym{NN}{NN}{Neural Network}
\newacronym{DNN}{DNN}{Deep Neural Network}

\newacronym{CNN}{CNN}{Convolutional Neural Network}
\newacronym{MCDNN}{MCDNN}{Multi-Column Deep Neural Network}

\newacronym{SSD}{SSD}{Single Shot Detector}
\newacronym{RCNN}{R-CNN}{Region CNN}


\newacronym{NMS}{NMS}{Non Maxima Suppression}


\newacronym{SIFT}{SIFT}{Scale Invariant Feature Transform}
\newacronym{SURF}{SURF}{Speeded Up Robust Features}

\newacronym{ALOI}{ALOI}{Amsterdam Library of Images}
\newacronym{MSR}{MSR}{Microsoft Research}
\newacronym{AMT}{AMT}{Amazon Mechanical Turk}

\newacronym{BOW}{BoW}{Bag of Words}
\newacronym{BOVW}{BoVW}{Bag of Visual Words}

\newacronym{ANN}{ANN}{Approximate Nearest Neighbour}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{ASGD}{ASGD}{Asynchronous Stochastic Gradient Descent}
\newacronym{LOO}{LOO}{Leave One Out}

\newacronym{NCA}{NCA}{Neighbourhood Components Analysis}
\newacronym{MEGM}{MEGM}{Mean square Error's Gradient Minimisation}

\newacronym{KNN}{kNN}{k-Nearest Neighbour}

\newacronym{MSE}{MSE}{Mean Squared Error}
\newacronym{LMNN}{LMNN}{Large Margin Nearest Neighbour}
\newacronym{NCM}{NCM}{Nearest Class Mean}

\newacronym{SVM}{SVM}{Support Vector Machine}

\newacronym{PCA}{PCA}{Principle Components Analysis}
\newacronym{DRLIM}{DrLIM}{Dimensionality Reduction by Learning an Invariant Mapping}
\newacronym{SGDR}{SGDR}{Stochastic Gradient Descent with Restarts}
 
\newacronym{GPU}{GPU}{Graphics Processing Unit} 
\newacronym{CPU}{CPU}{Central Processing Unit}

\newacronym{GHC}{GHC}{Glasgow Haskell Compiler}

\newacronym{GHCJS}{GHCJS}{GHC for JavaScript}

\newacronym{HTML}{HTML}{HyperText Markup Language}
\newacronym{SVG}{SVG}{Scaleable Vector Graphics}

\newacronym{HTTP}{HTTP}{HyperText Transfer Protocol}
\newacronym{JSON}{JSON}{JavaScript Object Notation}

\newacronym{ID}{ID}{Index of Difficulty}
\newacronym{AWS}{AWS}{Amazon Web Services}

\newacronym{CAT}{CAT}{Cumulative Annotation Time}
\newacronym{ROV}{ROV}{Remotely Operated Vehicle}


\acknowledgments{%
I will like to thank all the hedgehogs, without there help this
thesis would not be a reality.}

\textpages 

%\input{chapters/1_introduction.tex}
%\input{chapters/2_focus.tex}
%\input{chapters/3_metric_learning.tex}
%\input{chapters/4_bootstrap.tex}
%\input{chapters/5_design.tex}
%\input{chapters/6_object_detection.tex}
\input{chapters/7_annotation.tex}
%\input{chapters/8_future.tex}

% \begin{appendices}
% \input{chapters/9_datasets.tex}

% \end{appendices}

\bibliographystyle{unsrt}
\bibliography{references}
\end{document}