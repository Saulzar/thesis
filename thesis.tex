\documentclass[twoside]{styles/uocthesis}

\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage[toc,page]{appendix}

\usepackage[pdftex]{graphicx} %Loading the package
\graphicspath{{figures/}} %Setting the 
\usepackage{mathtools}
\usepackage{bm}
\usepackage[flushleft]{threeparttable}

\usepackage{adjustbox}
\usepackage[acronym]{glossaries}
\usepackage{booktabs}
\usepackage{styles/gnuplot-lua-tikz}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{enumitem}

\usepackage{csquotes}

\usepackage{todonotes}
\usepackage{float}
\usepackage{makecell} 
\usepackage{multirow}

\usepackage{url}
\usepackage[all]{nowidow}
\usepackage{siunitx}

\usepackage{hyperref}
\urlstyle{same}

 
\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}

\tolerance=9000
\pretolerance=5000
\emergencystretch=0pt

\righthyphenmin=4
\lefthyphenmin=4

\setlist{parsep=0pt,listparindent=\parindent}


\Title{Verification Based Annotation for Visual Recognition}
\Author{Oliver Batchelor}
\Year{2019}

\Supervisor{Prof. Richard Green}
\Department{Department of Computer Science}

\makeglossaries




\begin{document}

\prelimpages


\titlepage


\abstract{%

Applying deep learning and \gls{CNN}s to new domains usually implies a data collection and annotation problem. While several large datasets exist and provide a great deal of utility, there is a need to apply deep learning to new domains more easily, and to more easily experiment without the burden of spending a large amount of time upfront annotating data.

The major work in this thesis has been in evaluating and characterising proposed methods centred around the collaboration of human and a machine learning I term \gls{VBA}, intended for human-efficient annotation as well as rapid prototyping. A proposed difference to similar works, is the use of online training, as opposed to either (a) strong models trained on large data sets or (b) staged systems with alternating periods of annotation and training. 

Contrary to popular belief that \gls{CNN}s require much data and much training time, I demonstrate the opposite, using few images and also very little training time so that a \gls{CNN} can be trained to a level that provides genuine assistance to a human annotator. I propose methods for high-resolution object detection, which can improve accuracy and improve the speed of learning and study how noise and systematic bias degrade performance. 

I demonstrate the effectiveness of \gls{VBA} methods by annotating a variety of real-world image sets. I find it is especially effective in image sets with uniformity of object instances, reducing required annotation outright by $75$--$93\%$ on many datasets, and a further $10\%$ in several cases, using novel methods for utilising weakly confident detections.

One successful demonstration of \gls{VBA} is verified counting on images of Ad\'elie penguins and Weddell seals, where it has the promise of revolutionising the field. Counting takes a fraction of the effort and improves consistency compared to widely used methods such as crowdsourcing. Verification based methods offer immediate visual feedback and improved engagement, where the chore of annotating many images becomes the exciting task of teaching a machine to recognise the objects.

}

\tableofcontents



\printglossaries

\newacronym{VBA}{VBA}{Verification Based Annotation}
\newacronym{FCN}{FCN}{Fully Convolutional Network}
\newacronym{FPN}{FPN}{Feature Pyramid Network}


\newacronym{IID}{IID}{Independent and Identically Distributed}

\newacronym{IOU}{IoU}{Intersection over Union}
\newacronym{VOC}{VOC}{Visual Object Classes}
\newacronym{mIOU}{mIOU}{mean Intersection Over Union}

\newacronym{mAP}{mAP}{mean Average Precision}
\newacronym{AP}{AP}{Average Precision}


\newacronym{CRF}{CRF}{Conditional Random Field}

\newacronym{RELU}{ReLU}{Rectified Linear Unit}
\newacronym{PRELU}{PReLU}{Parameterised Rectified Linear Unit}
\newacronym{DECAF}{DeCAF}{Deep Convolutional Activation Feature}

\newacronym{FPN}{FPN}{Feature Pyramid Network}


\newacronym{NLL}{NLL}{Negative Loss Likelihood}
\newacronym{BCE}{BCE}{Binary Cross Entropy}
\newacronym{CE}{CE}{Cross Entropy}

\newacronym{EER}{EER}{Equal Error Rate}

 
\newacronym{COCO}{COCO}{Microsoft Common Objects in Context}
\newacronym{ILSVRC}{ILSVRC}{ImageNet Large Scale Visual Recognition Challenge}

\newacronym{NN}{NN}{Neural Network}
\newacronym{DNN}{DNN}{Deep Neural Network}

\newacronym{CNN}{CNN}{Convolutional Neural Network}
\newacronym{MCDNN}{MCDNN}{Multi-Column Deep Neural Network}

\newacronym{SSD}{SSD}{Single Shot Detector}
\newacronym{RCNN}{R-CNN}{Region CNN}

\newacronym{NMS}{NMS}{Non Maxima Suppression}


\newacronym{SIFT}{SIFT}{Scale Invariant Feature Transform}
\newacronym{SURF}{SURF}{Speeded Up Robust Features}

\newacronym{ALOI}{ALOI}{Amsterdam Library of Images}
\newacronym{MSR}{MSR}{Microsoft Research}
\newacronym{AMT}{AMT}{Amazon Mechanical Turk}

\newacronym{BOW}{BoW}{Bag of Words}
\newacronym{BOVW}{BoVW}{Bag of Visual Words}

\newacronym{ANN}{ANN}{Approximate Nearest Neighbour}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{ASGD}{ASGD}{Asynchronous Stochastic Gradient Descent}
\newacronym{LOO}{LOO}{Leave One Out}

\newacronym{NCA}{NCA}{Neighbourhood Components Analysis}
\newacronym{MEGM}{MEGM}{Mean square Error's Gradient Minimisation}

\newacronym{KNN}{kNN}{k-Nearest Neighbour}

\newacronym{MSE}{MSE}{Mean Squared Error}
\newacronym{LMNN}{LMNN}{Large Margin Nearest Neighbour}
\newacronym{NCM}{NCM}{Nearest Class Mean}

\newacronym{SVM}{SVM}{Support Vector Machine}

\newacronym{PCA}{PCA}{Principle Components Analysis}
\newacronym{DRLIM}{DrLIM}{Dimensionality Reduction by Learning an Invariant Mapping}
\newacronym{SGDR}{SGDR}{Stochastic Gradient Descent with Restarts}


\newacronym{GPU}{GPU}{Graphics Processing Unit} 
\newacronym{CPU}{CPU}{Central Processing Unit}

\newacronym{API}{API}{Application Programming Interface} 

\newacronym{GHC}{GHC}{Glasgow Haskell Compiler}
\newacronym{GHCJS}{GHCJS}{GHC for JavaScript}

\newacronym{HTML}{HTML}{HyperText Markup Language}
\newacronym{SVG}{SVG}{Scaleable Vector Graphics}

\newacronym{HTTP}{HTTP}{HyperText Transfer Protocol}
\newacronym{JSON}{JSON}{JavaScript Object Notation}

\newacronym{ID}{ID}{Index of Difficulty}
\newacronym{AWS}{AWS}{Amazon Web Services}

\newacronym{CAT}{CAT}{Cumulative Annotation Time}
\newacronym{ROV}{ROV}{Remotely Operated Vehicle}


% \acknowledgments{}

\textpages 

\setcounter{chapter}{0}
\input{chapters/1_introduction.tex}

\setcounter{chapter}{1}
\input{chapters/2_focus.tex}

\setcounter{chapter}{2}
\input{chapters/3_metric_learning.tex}

\setcounter{chapter}{3}
\input{chapters/4_bootstrap.tex}

\setcounter{chapter}{4}
\input{chapters/5_design.tex}

\setcounter{chapter}{5}
\input{chapters/6_object_detection.tex}

\setcounter{chapter}{6}
\input{chapters/7_annotation.tex}

\setcounter{chapter}{7}
\input{chapters/8_conclusion.tex}

% \setcounter{chapter}{10}
% \input{chapters/10_poster.tex}


\begin{appendices}
\input{chapters/9_datasets.tex}
\end{appendices}



\bibliographystyle{unsrt}
\bibliography{references}
\end{document} 